"""
DOM/ì •ì /RSS íƒìƒ‰ í…œí”Œë¦¿ ìŠ¤í¬ë¦½íŠ¸
================================
ì •ì  HTML(ê°€ëŠ¥í•˜ë©´ BeautifulSoup)ë¡œ ë¨¼ì € êµ¬ì¡°ë¥¼ íƒìƒ‰í•˜ê³ ,
RSS/Atom(XML)ë¡œ íŒë‹¨ë˜ë©´ í”¼ë“œ ë°©ì‹ìœ¼ë¡œ ë¶„ê¸° ì²˜ë¦¬í•©ë‹ˆë‹¤.
ì •ì  HTMLë§Œìœ¼ë¡œ ë¶€ì¡±(JS ë Œë”ë§/í´ë¦­/ìŠ¤í¬ë¡¤ ë“± ë™ì  ìš”ì†Œ í•„ìš”)í•˜ë‹¤ê³  íŒë‹¨ë  ë•Œë§Œ
Playwright ê¸°ë°˜ DOM íƒìƒ‰ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.

ì‚¬ìš©ë²•:
    python explore_template.py <URL> [output_path]

ì˜ˆì‹œ:
    python explore_template.py https://example.com
    python explore_template.py https://example.com ./result.json
"""

from __future__ import annotations

import argparse
import json
import os
import re
import sys
import urllib.robotparser
from datetime import datetime
from typing import Optional
from urllib.error import HTTPError, URLError
from urllib.parse import urljoin, urlparse
from urllib.request import Request, urlopen

try:
    from bs4 import BeautifulSoup
except ImportError:  # pragma: no cover
    BeautifulSoup = None

try:
    import xml.etree.ElementTree as ET
except ImportError:  # pragma: no cover
    ET = None

def _find_repo_root(start_path: str) -> str:
    """ì›Œí¬ìŠ¤í˜ì´ìŠ¤ ë£¨íŠ¸ ì°¾ê¸°"""
    from pathlib import Path
    start = Path(start_path).resolve()
    for candidate in [start, *start.parents]:
        if (candidate / ".python-version").exists() or (candidate / "AGENTS.md").exists() or (candidate / ".git").exists():
            return str(candidate)
    return str(start)


def select_output_folder() -> Optional[str]:
    """
    ì‚¬ìš©ìì—ê²Œ ê²°ê³¼ë¥¼ ì €ì¥í•  í´ë”ë¥¼ ì„ íƒí•˜ë„ë¡ í•¨ (ì¸í„°ë™í‹°ë¸Œ)
    """
    from pathlib import Path

    script_path = Path(__file__).resolve()
    repo_root = Path(_find_repo_root(str(script_path.parent)))

    print("\n" + "="*50)
    print("ğŸ“ íƒìƒ‰ ê²°ê³¼ë¥¼ ì €ì¥í•  í´ë”ë¥¼ ì„ íƒí•˜ì„¸ìš”")
    print("="*50)

    workspace_folders = [
        ("30-collected/31-web-scraps", "ì›¹ ìŠ¤í¬ë© (ê¶Œì¥)"),
        ("30-collected/33-news", "ë‰´ìŠ¤ ìˆ˜ì§‘"),
        ("10-working", "ì§„í–‰ ì¤‘ì¸ ì‘ì—…"),
        ("40-archive", "ì•„ì¹´ì´ë¸Œ"),
    ]

    available = []
    for folder, desc in workspace_folders:
        folder_path = repo_root / folder
        if folder_path.exists():
            file_count = len([f for f in folder_path.iterdir() if f.is_file() and not f.name.startswith('.')])
            available.append((folder, desc, file_count, True))
        else:
            available.append((folder, desc, 0, False))

    for i, (folder, desc, count, exists) in enumerate(available, 1):
        status = f"{count}ê°œ íŒŒì¼" if exists else "ìƒˆë¡œ ìƒì„±"
        print(f"  {i}. {folder:<30} ({desc}, {status})")

    print(f"  {len(available)+1}. ì§ì ‘ ì…ë ¥")
    print("-"*50)

    while True:
        try:
            sel = input("ë²ˆí˜¸ ì„ íƒ (0: ì·¨ì†Œ): ").strip()
            if sel == '0':
                return None
            idx = int(sel)

            if 1 <= idx <= len(available):
                selected = repo_root / available[idx-1][0]
                selected.mkdir(parents=True, exist_ok=True)
                print(f"\n  âœ“ ì„ íƒë¨: {selected}")
                return str(selected)
            elif idx == len(available) + 1:
                custom = input("í´ë” ê²½ë¡œ ì…ë ¥ (ìƒëŒ€/ì ˆëŒ€): ").strip()
                if custom:
                    custom_path = Path(custom)
                    if not custom_path.is_absolute():
                        custom_path = repo_root / custom_path
                    custom_path.mkdir(parents=True, exist_ok=True)
                    return str(custom_path.resolve())
        except ValueError:
            pass

    return None


def extract_domain_name(url):
    """URLì—ì„œ ë„ë©”ì¸ëª… ì¶”ì¶œ"""
    parsed = urlparse(url)
    domain = parsed.netloc.replace("www.", "")
    # ì ì„ ì–¸ë”ìŠ¤ì½”ì–´ë¡œ ë³€í™˜
    return domain.replace(".", "_")


TAB_SELECTORS = [
    "[role='tab']",
    "[role='tablist'] > *",
    ".tab",
    ".tabs > *",
    "[data-tab]",
    ".nav-tab",
    ".tab-button",
]

CARD_SELECTORS = [
    ".card",
    ".item",
    ".study-card",
    ".post-card",
    ".product-card",
    "[class*='card']",
    "[class*='item']",
    "article",
    ".list-item",
    "[data-id]",
]

PAGINATION_SELECTORS = [
    ".pagination",
    "[class*='paging']",
    ".page-nav",
    "nav[aria-label*='page']",
]

MODAL_SELECTORS = [
    "[role='dialog']",
    ".modal",
    "[class*='modal']",
    "[class*='popup']",
]


def default_user_agent() -> str:
    return "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0 Safari/537.36"


def resolve_output_path(url: str, output_path: Optional[str]) -> str:
    if output_path:
        return output_path

    domain = extract_domain_name(url)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"{domain}_structure_{timestamp}.json"

    # í™˜ê²½ë³€ìˆ˜ í™•ì¸
    env_output_dir = os.getenv('WEBSCRAPER_OUTPUT_DIR')
    if env_output_dir:
        output_dir = env_output_dir
    else:
        # ì¸í„°ë™í‹°ë¸Œ í´ë” ì„ íƒ
        output_dir = select_output_folder()
        if not output_dir:
            print("[ì·¨ì†Œ] íƒìƒ‰ì´ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.")
            sys.exit(0)

    return os.path.join(output_dir, filename)


def safe_makedirs_for_file(path: str) -> None:
    directory = os.path.dirname(path)
    if directory:
        os.makedirs(directory, exist_ok=True)


def fetch_url_content(url: str, user_agent: str, timeout_ms: int, accept: str = "*/*") -> dict:
    req = Request(
        url,
        headers={
            "User-Agent": user_agent,
            "Accept": accept,
        },
    )
    timeout_sec = max(1, int(timeout_ms / 1000))
    with urlopen(req, timeout=timeout_sec) as res:
        body = res.read()
        return {
            "final_url": res.geturl(),
            "status_code": res.getcode(),
            "content_type": res.headers.get("Content-Type", ""),
            "body": body,
        }


def decode_body(body: bytes, content_type: str) -> str:
    charset = None
    match = re.search(r"charset=([^\s;]+)", content_type or "", flags=re.IGNORECASE)
    if match:
        charset = match.group(1).strip("\"'")
    for enc in [charset, "utf-8", "cp949", "euc-kr"]:
        if not enc:
            continue
        try:
            return body.decode(enc, errors="replace")
        except Exception:
            continue
    return body.decode("utf-8", errors="replace")


def looks_like_xml_or_feed(url: str, content_type: str, body: bytes) -> bool:
    lowered_ct = (content_type or "").lower()
    if any(token in lowered_ct for token in ["application/rss+xml", "application/atom+xml", "application/xml", "text/xml"]):
        return True

    lowered_url = (url or "").lower()
    if lowered_url.endswith((".xml", ".rss", ".atom")) or any(token in lowered_url for token in ["/rss", "/feed", "rss?"]):
        return True

    head = body[:2048].lstrip()
    head_text = ""
    try:
        head_text = head.decode("utf-8", errors="ignore").lower()
    except Exception:
        head_text = ""

    if head_text.startswith("<?xml") or head_text.startswith("<rss") or head_text.startswith("<feed") or "<rss" in head_text or "<feed" in head_text:
        return True

    return False


def extract_feed_links_from_html(html_text: str, base_url: str) -> list[dict]:
    feeds: list[dict] = []

    if BeautifulSoup:
        soup = BeautifulSoup(html_text, "html.parser")
        for link in soup.find_all("link"):
            rel = link.get("rel") or []
            if isinstance(rel, str):
                rel = [rel]
            rel = [str(r).lower() for r in rel]
            if "alternate" not in rel:
                continue

            href = link.get("href")
            ftype = (link.get("type") or "").lower()
            if not href:
                continue

            if any(token in ftype for token in ["application/rss+xml", "application/atom+xml", "application/xml", "text/xml"]):
                feeds.append(
                    {
                        "url": urljoin(base_url, href),
                        "type": ftype or "unknown",
                        "title": link.get("title") or "",
                        "source": "link[rel=alternate]",
                    }
                )
    else:
        pattern = re.compile(r'<link[^>]+rel=["\']?alternate["\']?[^>]*>', flags=re.IGNORECASE)
        href_re = re.compile(r'href=["\']([^"\']+)["\']', flags=re.IGNORECASE)
        type_re = re.compile(r'type=["\']([^"\']+)["\']', flags=re.IGNORECASE)
        title_re = re.compile(r'title=["\']([^"\']+)["\']', flags=re.IGNORECASE)
        for m in pattern.finditer(html_text):
            tag = m.group(0)
            href_m = href_re.search(tag)
            type_m = type_re.search(tag)
            if not href_m:
                continue
            ftype = (type_m.group(1) if type_m else "").lower()
            if not any(token in ftype for token in ["application/rss+xml", "application/atom+xml", "application/xml", "text/xml"]):
                continue
            title_m = title_re.search(tag)
            feeds.append(
                {
                    "url": urljoin(base_url, href_m.group(1)),
                    "type": ftype or "unknown",
                    "title": title_m.group(1) if title_m else "",
                    "source": "regex",
                }
            )

    seen = set()
    unique: list[dict] = []
    for f in feeds:
        u = f.get("url")
        if not u or u in seen:
            continue
        seen.add(u)
        unique.append(f)
    return unique


def strip_xml_ns(tag: str) -> str:
    if "}" in tag:
        return tag.split("}", 1)[1]
    return tag


def parse_feed_preview(xml_bytes: bytes) -> dict:
    if not ET:
        return {"status": "error", "error": "xml.etree.ElementTreeë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}

    try:
        root = ET.fromstring(xml_bytes)
    except Exception as e:
        return {"status": "error", "error": f"XML íŒŒì‹± ì‹¤íŒ¨: {e}"}

    root_tag = strip_xml_ns(root.tag).lower()

    if root_tag == "rss" or root_tag == "rdf":
        channel = None
        for child in root:
            if strip_xml_ns(child.tag).lower() == "channel":
                channel = child
                break

        item_nodes = []
        if channel is not None:
            item_nodes = [c for c in channel if strip_xml_ns(c.tag).lower() == "item"]
        else:
            item_nodes = [c for c in root.iter() if strip_xml_ns(c.tag).lower() == "item"]

        sample_items = []
        for item in item_nodes[:10]:
            title = ""
            link = ""
            pub_date = ""
            for c in item:
                t = strip_xml_ns(c.tag).lower()
                if t == "title":
                    title = (c.text or "").strip()
                elif t == "link":
                    link = (c.text or "").strip()
                elif t in ("pubdate", "date"):
                    pub_date = (c.text or "").strip()
            sample_items.append({"title": title, "link": link, "date": pub_date})

        return {"status": "success", "feed_type": "rss", "items_count": len(item_nodes), "sample_items": sample_items}

    if root_tag == "feed":
        entry_nodes = [c for c in root if strip_xml_ns(c.tag).lower() == "entry"]
        sample_items = []
        for entry in entry_nodes[:10]:
            title = ""
            link = ""
            updated = ""
            for c in entry:
                t = strip_xml_ns(c.tag).lower()
                if t == "title":
                    title = (c.text or "").strip()
                elif t == "link":
                    href = c.attrib.get("href")
                    if href:
                        link = href.strip()
                elif t in ("updated", "published"):
                    updated = (c.text or "").strip()
            sample_items.append({"title": title, "link": link, "date": updated})

        return {"status": "success", "feed_type": "atom", "items_count": len(entry_nodes), "sample_items": sample_items}

    return {"status": "success", "feed_type": "xml", "items_count": 0, "sample_items": []}


def try_fetch_feed_candidate(feed_url: str, user_agent: str, timeout_ms: int) -> Optional[dict]:
    """
    HTMLì—ì„œ ë°œê²¬í•œ RSS/Atom í›„ë³´ URLì„ ì‹¤ì œë¡œ ìš”ì²­í•´ë³´ê³ , íŒŒì‹±ì´ ê°€ëŠ¥í•˜ë©´ feed ì •ë³´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    ì‹¤íŒ¨í•˜ë©´ Noneì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    try:
        fetched = fetch_url_content(
            feed_url,
            user_agent,
            timeout_ms,
            accept="application/rss+xml,application/atom+xml,application/xml,text/xml,*/*",
        )
        final_url = fetched.get("final_url") or feed_url
        content_type = fetched.get("content_type") or ""
        body = fetched.get("body") or b""

        preview = parse_feed_preview(body)
        if preview.get("status") != "success":
            return None

        items_count = preview.get("items_count")
        try:
            count_int = int(items_count)
        except Exception:
            count_int = 0

        sample_items = preview.get("sample_items") or []
        if count_int <= 0 and not sample_items:
            return None

        return {"url": final_url, "content_type": content_type, **preview}
    except Exception:
        return None


def detect_spa_from_html(html_text: str) -> dict:
    lowered = (html_text or "").lower()
    return {
        "hasReact": any(token in lowered for token in ["data-reactroot", "data-reactid", "react-dom", "__next_data__", "id=\"__next\""]),
        "hasVue": any(token in lowered for token in ["data-v-", "vue", "nuxt"]),
        "hasAngular": any(token in lowered for token in ["ng-app", "ng-controller", "<app-root", "angular"]),
    }


def analyze_static_html(html_text: str, base_url: str) -> dict:
    if not BeautifulSoup:
        return {
            "status": "error",
            "errors": [
                "ì •ì  HTML ë¶„ì„ì„ ìœ„í•´ `beautifulsoup4` ì„¤ì¹˜ê°€ í•„ìš”í•©ë‹ˆë‹¤. (ì˜ˆ: python -m pip install beautifulsoup4 soupsieve)"
            ],
        }

    soup = BeautifulSoup(html_text, "html.parser")

    title = ""
    try:
        if soup.title and soup.title.string:
            title = soup.title.string.strip()
    except Exception:
        title = ""

    selectors = {
        "tabs": [],
        "cards": [],
        "links": [],
        "buttons": [],
        "forms": [],
        "tables": [],
        "lists": [],
        "images": [],
        "pagination": [],
    }

    # íƒ­/ë„¤ë¹„ê²Œì´ì…˜ íƒìƒ‰(ì •ì )
    for sel in TAB_SELECTORS:
        try:
            tabs = soup.select(sel)
            if tabs and len(tabs) > 0:
                selectors["tabs"].append(
                    {
                        "selector": sel,
                        "count": len(tabs),
                        "texts": [t.get_text(" ", strip=True)[:50] for t in tabs[:5]],
                    }
                )
        except Exception:
            pass

    # ì¹´ë“œ/ì•„ì´í…œ íƒìƒ‰(ì •ì )
    for sel in CARD_SELECTORS:
        try:
            cards = soup.select(sel)
            if not cards or len(cards) < 2:
                continue
            if len(cards) > 200:
                continue

            first = cards[0]
            sample_text = first.get_text(" ", strip=True)[:100]
            has_link = first.select_one("a[href]") is not None
            has_image = first.select_one("img") is not None

            data_attributes = []
            try:
                for k, v in (first.attrs or {}).items():
                    if not str(k).startswith("data-"):
                        continue
                    if v is None:
                        continue
                    data_attributes.append({str(k): str(v)[:80]})
                    if len(data_attributes) >= 6:
                        break
            except Exception:
                pass

            selectors["cards"].append(
                {
                    "selector": sel,
                    "count": len(cards),
                    "sample_text": sample_text,
                    "has_link": has_link,
                    "has_image": has_image,
                    "data_attributes": data_attributes,
                }
            )
        except Exception:
            pass

    # ë§í¬ íŒ¨í„´
    try:
        links = soup.select("a[href]")
        unique_patterns = set()
        for link in links[:200]:
            href = link.get("href") or ""
            text = link.get_text(" ", strip=True)[:30]
            if not href or href.startswith("#") or href.lower().startswith("javascript"):
                continue
            abs_href = urljoin(base_url, href)
            pattern = re.sub(r"/[a-zA-Z0-9_-]{10,}", "/*", abs_href)
            pattern = re.sub(r"/\\d+", "/*", pattern)
            if pattern not in unique_patterns and len(unique_patterns) < 10:
                unique_patterns.add(pattern)
                selectors["links"].append({"pattern": pattern, "sample_href": abs_href[:200], "sample_text": text})
    except Exception:
        pass

    # ë²„íŠ¼
    try:
        for btn in soup.select("button, [role='button'], .btn, [class*='button']")[:20]:
            text = btn.get_text(" ", strip=True)[:30]
            if text:
                selectors["buttons"].append(
                    {
                        "text": text,
                        "tag": btn.name,
                        "classes": " ".join(btn.get("class") or []),
                    }
                )
    except Exception:
        pass

    # í˜ì´ì§€ë„¤ì´ì…˜
    try:
        for sel in PAGINATION_SELECTORS:
            try:
                pag = soup.select_one(sel)
            except Exception:
                pag = None
            if pag:
                selectors["pagination"].append({"selector": sel, "exists": True})
    except Exception:
        pass

    spa_framework = detect_spa_from_html(html_text)
    is_spa = any(spa_framework.values())

    # JS í•„ìš” ë¬¸êµ¬(noscript) íƒì§€
    noscript_text = ""
    try:
        ns = soup.find("noscript")
        if ns:
            noscript_text = ns.get_text(" ", strip=True).lower()
    except Exception:
        pass
    has_js_required_message = any(token in noscript_text for token in ["javascript", "ìë°”ìŠ¤í¬ë¦½íŠ¸", "enable", "í™œì„±í™”"])

    # íœ´ë¦¬ìŠ¤í‹±: ë™ì  í•„ìš”ì„± ì¶”ì •
    try:
        text_len = len(soup.get_text(" ", strip=True))
    except Exception:
        text_len = 0
    try:
        script_count = len(soup.find_all("script"))
    except Exception:
        script_count = 0

    has_cards = bool(selectors.get("cards"))
    needs_playwright = bool(is_spa or has_js_required_message or (not has_cards and script_count >= 10) or (text_len < 200 and script_count >= 20))

    reason_parts = []
    if is_spa:
        reason_parts.append("SPA(React/Vue/Angular/Next ë“±) ì§•í›„")
    if has_js_required_message:
        reason_parts.append("noscriptì— 'JavaScript í•„ìš”' ë¬¸êµ¬")
    if not has_cards and script_count >= 10:
        reason_parts.append("ì¹´ë“œ íŒ¨í„´ ë¯¸íƒì§€ + script ë‹¤ìˆ˜")
    if text_len < 200 and script_count >= 20:
        reason_parts.append("í…ìŠ¤íŠ¸ ì ìŒ + script ë§¤ìš° ë§ìŒ")

    return {
        "status": "success",
        "page_info": {"title": title, "url": base_url, "has_changed_url": False},
        "selectors": selectors,
        "dynamic_features": {
            "has_infinite_scroll": False,
            "has_lazy_loading": False,
            "has_modals": False,
            "has_tabs": bool(selectors.get("tabs")),
            "is_spa": bool(is_spa),
            "spa_framework": spa_framework,
        },
        "static_probe": {
            "text_length": text_len,
            "script_count": script_count,
            "needs_playwright": needs_playwright,
            "reason": ", ".join(reason_parts) if reason_parts else "",
        },
    }


def robots_check_and_confirm(url: str, user_agent: str, timeout_ms: int, skip_prompt: bool) -> dict:
    parsed = urlparse(url)
    robots_url = f"{parsed.scheme}://{parsed.netloc}/robots.txt"

    info = {
        "robots_url": robots_url,
        "can_fetch": None,
        "checked": False,
        "user_agent": user_agent,
        "error": "",
    }

    try:
        req = Request(robots_url, headers={"User-Agent": user_agent, "Accept": "text/plain,*/*"})
        timeout_sec = max(1, int(timeout_ms / 1000))
        with urlopen(req, timeout=timeout_sec) as res:
            robots_text = decode_body(res.read(), res.headers.get("Content-Type", ""))

        rp = urllib.robotparser.RobotFileParser()
        rp.set_url(robots_url)
        rp.parse(robots_text.splitlines())
        can_fetch = rp.can_fetch(user_agent or "*", url)
        info["can_fetch"] = bool(can_fetch)
        info["checked"] = True
    except Exception as e:
        info["error"] = str(e)
        return info

    if info["can_fetch"] is True:
        return info

    print("\n" + "=" * 60)
    print("[ê²½ê³ ] robots.txt ì •ì±…ìƒ í˜„ì¬ URL ê²½ë¡œëŠ” ìŠ¤í¬ë˜í•‘/ìë™ìˆ˜ì§‘ì´ ê¸ˆì§€ë˜ì–´ ìˆì„ ê°€ëŠ¥ì„±ì´ í½ë‹ˆë‹¤.")
    print("[ì¤‘ë‹¨ ê¶Œì¥] í—ˆê°€/ê·¼ê±° ì—†ì´ ì§„í–‰í•˜ë©´ ì €ì‘ê¶Œ ì¹¨í•´, ì•½ê´€ ìœ„ë°˜, ì—…ë¬´ë°©í•´/ì»´í“¨í„°ì‹œìŠ¤í…œ ì¹¨í•´ ë“± ë²•ì Â·ìœ¤ë¦¬ì  ë¦¬ìŠ¤í¬ê°€ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    print("[ì§ˆë¬¸] ê·¸ëŸ¼ì—ë„ ë¶ˆêµ¬í•˜ê³  ì§„í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N)")
    print("=" * 60)

    if skip_prompt:
        print("[ì•ˆë‚´] --skip-robots-prompt ì˜µì…˜ìœ¼ë¡œ í™•ì¸ì„ ìƒëµí–ˆìŠµë‹ˆë‹¤. ì‚¬ìš©ì ì±…ì„ í•˜ì— ì§„í–‰í•©ë‹ˆë‹¤.")
        return info

    ans = input("ì§„í–‰ í™•ì¸ (y/N): ").strip().lower()
    if ans not in ("y", "yes"):
        raise SystemExit("ì‚¬ìš©ì í™•ì¸ì´ ì—†ì–´ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")

    return info


def explore_page_structure(url, output_path=None, headless=True, timeout=30000, mode: str = "auto", user_agent: Optional[str] = None, skip_robots_prompt: bool = False):
    """
    í˜ì´ì§€ êµ¬ì¡°ë¥¼ íƒìƒ‰í•˜ê³  ì£¼ìš” ì…€ë ‰í„°ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.

    - (ê¸°ë³¸) ì •ì  HTML(BeautifulSoup)ë¡œ ë¨¼ì € íƒìƒ‰
    - RSS/Atom(XML)ë¡œ íŒë‹¨ë˜ë©´ feed ë°©ì‹ìœ¼ë¡œ ë¶„ê¸°
    - ì •ì  HTMLë§Œìœ¼ë¡œ ë¶€ì¡±(JS ë Œë”ë§/í´ë¦­/ìŠ¤í¬ë¡¤ ë“±)í•˜ë‹¤ê³  íŒë‹¨ë  ë•Œë§Œ Playwright íƒìƒ‰ ì‚¬ìš©

    Args:
        url: íƒìƒ‰í•  URL
        output_path: ê²°ê³¼ ì €ì¥ ê²½ë¡œ (ì—†ìœ¼ë©´ ìë™ ìƒì„±)
        headless: ë¸Œë¼ìš°ì € í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œ (ê¸°ë³¸: True)
        timeout: í˜ì´ì§€ ë¡œë“œ íƒ€ì„ì•„ì›ƒ (ms)
        mode: auto|static|rss|playwright
        user_agent: HTTP ìš”ì²­ User-Agent (ê¸°ë³¸: Windows Chrome UA)
        skip_robots_prompt: robots ì°¨ë‹¨ ì‹œ ì‚¬ìš©ì í™•ì¸ì„ ìƒëµ

    Returns:
        dict: íƒìƒ‰ ê²°ê³¼
    """

    if not user_agent:
        user_agent = default_user_agent()

    output_path = resolve_output_path(url, output_path)
    safe_makedirs_for_file(output_path)

    # robots.txt ì°¸ê³ (ì°¨ë‹¨ ì‹œ ê°•í•œ ê²½ê³  + ì‚¬ìš©ì í™•ì¸)
    robots_info = robots_check_and_confirm(url, user_agent, timeout, skip_robots_prompt)

    result = {
        "url": url,
        "timestamp": datetime.now().isoformat(),
        "status": "pending",
        "analysis_method": "",
        "page_info": {},
        "selectors": {
            "tabs": [],
            "cards": [],
            "links": [],
            "buttons": [],
            "forms": [],
            "tables": [],
            "lists": [],
            "images": [],
            "pagination": [],
        },
        "dynamic_features": {
            "has_infinite_scroll": False,
            "has_lazy_loading": False,
            "has_modals": False,
            "has_tabs": False,
            "is_spa": False,
        },
        "recommended_selectors": [],
        "errors": [],
        "robots": robots_info,
        "discovered_feeds": [],
    }

    print(f"\n{'='*60}")
    print(f"íƒìƒ‰ ì‹œì‘: {url}")
    print(f"ëª¨ë“œ: {mode}")
    print(f"{'='*60}\n")

    # 1) ì •ì  fetch & RSS ë¶„ê¸°
    if mode in ("auto", "static", "rss"):
        try:
            fetched = fetch_url_content(url, user_agent, timeout)
            final_url = fetched.get("final_url") or url
            content_type = fetched.get("content_type") or ""
            body = fetched.get("body") or b""
            body_text = decode_body(body, content_type)

            result["http"] = {
                "final_url": final_url,
                "status_code": fetched.get("status_code"),
                "content_type": content_type,
            }
            result["page_info"] = {"title": "", "url": final_url, "has_changed_url": final_url != url}

            if looks_like_xml_or_feed(final_url, content_type, body):
                result["analysis_method"] = "rss"
                result["feed"] = {"url": final_url, "content_type": content_type, **parse_feed_preview(body)}
                result["status"] = "success" if result["feed"].get("status") == "success" else "error"
                with open(output_path, "w", encoding="utf-8") as f:
                    json.dump(result, f, ensure_ascii=False, indent=2)

                print("[ì™„ë£Œ] RSS/Atom(XML)ë¡œ íŒë‹¨ë˜ì–´ feed ë°©ì‹ìœ¼ë¡œ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                print(f"  - JSON: {output_path}")
                return result

            result["discovered_feeds"] = extract_feed_links_from_html(body_text, final_url)

            # RSS ê°•ì œ ëª¨ë“œ: HTMLì—ì„œ í”¼ë“œ ë§í¬ë¥¼ ì°¾ì€ ë’¤ ì‹¤ì œ í”¼ë“œë¥¼ ê°€ì ¸ì™€ íŒŒì‹±ì„ ì‹œë„
            if mode == "rss":
                if not result["discovered_feeds"]:
                    result["analysis_method"] = "rss"
                    result["status"] = "error"
                    result["errors"].append("HTMLì—ì„œ RSS/Atom í”¼ë“œ ë§í¬ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                    with open(output_path, "w", encoding="utf-8") as f:
                        json.dump(result, f, ensure_ascii=False, indent=2)
                    return result

                for cand in result["discovered_feeds"][:3]:
                    feed_url = cand.get("url")
                    if not feed_url:
                        continue
                    feed_info = try_fetch_feed_candidate(str(feed_url), user_agent, timeout)
                    if feed_info:
                        result["analysis_method"] = "rss"
                        result["feed"] = feed_info
                        result["status"] = "success"
                        with open(output_path, "w", encoding="utf-8") as f:
                            json.dump(result, f, ensure_ascii=False, indent=2)
                        return result

                result["analysis_method"] = "rss"
                result["status"] = "error"
                result["errors"].append("RSS/Atom í›„ë³´ëŠ” ë°œê²¬í–ˆì§€ë§Œ í”¼ë“œë¥¼ ê°€ì ¸ì˜¤ê±°ë‚˜ íŒŒì‹±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                with open(output_path, "w", encoding="utf-8") as f:
                    json.dump(result, f, ensure_ascii=False, indent=2)
                return result

            static_result = analyze_static_html(body_text, final_url)
            if static_result.get("status") == "success":
                result["analysis_method"] = "static_html"
                result["page_info"] = static_result.get("page_info") or result["page_info"]
                result["selectors"] = static_result.get("selectors") or result["selectors"]
                result["dynamic_features"] = static_result.get("dynamic_features") or result["dynamic_features"]
                result["static_probe"] = static_result.get("static_probe") or {}
                result["status"] = "success"

                # auto ëª¨ë“œì—ì„œ ë™ì  í•„ìš” íŒë‹¨ì´ë©´ Playwrightë¡œ ì „í™˜
                if mode == "auto" and result.get("static_probe", {}).get("needs_playwright"):
                    # ì •ì  ë¶„ì„ì—ì„œ ë™ì  í•„ìš”ë¡œ íŒë‹¨ë˜ë”ë¼ë„, RSS/Atomì´ ìˆìœ¼ë©´ í”¼ë“œë¥¼ ìš°ì„  ì‹œë„
                    for cand in result.get("discovered_feeds", [])[:3]:
                        feed_url = cand.get("url")
                        if not feed_url:
                            continue
                        feed_info = try_fetch_feed_candidate(str(feed_url), user_agent, timeout)
                        if feed_info:
                            result["analysis_method"] = "rss"
                            result["feed"] = feed_info
                            result["status"] = "success"
                            with open(output_path, "w", encoding="utf-8") as f:
                                json.dump(result, f, ensure_ascii=False, indent=2)
                            return result

                    print("[ì•ˆë‚´] ì •ì  HTMLë§Œìœ¼ë¡œëŠ” ë¶€ì¡±(ë™ì  í•„ìš”)í•˜ë‹¤ê³  íŒë‹¨ë˜ì–´ Playwright íƒìƒ‰ìœ¼ë¡œ ì „í™˜í•©ë‹ˆë‹¤.")
                    mode = "playwright"
                else:
                    with open(output_path, "w", encoding="utf-8") as f:
                        json.dump(result, f, ensure_ascii=False, indent=2)

                    print("[ì™„ë£Œ] ì •ì  HTML ê¸°ë°˜ íƒìƒ‰ì„ ë§ˆì¹©ë‹ˆë‹¤.")
                    if result["discovered_feeds"]:
                        print(f"  - RSS/Atom í›„ë³´: {len(result['discovered_feeds'])}ê°œ (discovered_feeds ì°¸ê³ )")
                    print(f"  - JSON: {output_path}")
                    return result
            else:
                result["errors"].extend(static_result.get("errors") or [])
                if mode in ("static", "rss"):
                    result["analysis_method"] = "static_html"
                    result["status"] = "error"
                    with open(output_path, "w", encoding="utf-8") as f:
                        json.dump(result, f, ensure_ascii=False, indent=2)
                    return result

                # ì •ì  ë¶„ì„ ì‹¤íŒ¨ ì‹œì—ë„ RSS/Atom í›„ë³´ê°€ ìˆìœ¼ë©´ ìš°ì„  ì‹œë„
                for cand in result.get("discovered_feeds", [])[:3]:
                    feed_url = cand.get("url")
                    if not feed_url:
                        continue
                    feed_info = try_fetch_feed_candidate(str(feed_url), user_agent, timeout)
                    if feed_info:
                        result["analysis_method"] = "rss"
                        result["feed"] = feed_info
                        result["status"] = "success"
                        with open(output_path, "w", encoding="utf-8") as f:
                            json.dump(result, f, ensure_ascii=False, indent=2)
                        return result

                print("[ì•ˆë‚´] ì •ì  ë¶„ì„ ì‹¤íŒ¨ë¡œ Playwright íƒìƒ‰ìœ¼ë¡œ ì „í™˜í•©ë‹ˆë‹¤.")
                mode = "playwright"

        except (HTTPError, URLError) as e:
            result["errors"].append(f"ì •ì  HTTP fetch ì‹¤íŒ¨: {e}")
            if mode in ("static", "rss"):
                result["analysis_method"] = "static_html" if mode == "static" else "rss"
                result["status"] = "error"
                with open(output_path, "w", encoding="utf-8") as f:
                    json.dump(result, f, ensure_ascii=False, indent=2)
                return result
            print("[ì•ˆë‚´] ì •ì  fetch ì‹¤íŒ¨ë¡œ Playwright íƒìƒ‰ìœ¼ë¡œ ì „í™˜í•©ë‹ˆë‹¤.")
            mode = "playwright"
        except Exception as e:
            result["errors"].append(f"ì •ì  ë¶„ì„ ì¤‘ ì˜ˆì™¸: {e}")
            if mode in ("static", "rss"):
                result["analysis_method"] = "static_html" if mode == "static" else "rss"
                result["status"] = "error"
                with open(output_path, "w", encoding="utf-8") as f:
                    json.dump(result, f, ensure_ascii=False, indent=2)
                return result
            print("[ì•ˆë‚´] ì •ì  ë¶„ì„ ì˜ˆì™¸ë¡œ Playwright íƒìƒ‰ìœ¼ë¡œ ì „í™˜í•©ë‹ˆë‹¤.")
            mode = "playwright"

    # 2) Playwright íƒìƒ‰(ë™ì  í•„ìš” ì‹œ)
    if mode == "playwright":
        try:
            from playwright.sync_api import sync_playwright  # type: ignore
        except Exception as e:
            result["analysis_method"] = "playwright"
            result["status"] = "error"
            result["errors"].append(f"Playwrightë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
            with open(output_path, "w", encoding="utf-8") as f:
                json.dump(result, f, ensure_ascii=False, indent=2)
            return result

        result["analysis_method"] = "playwright"
        result["selectors"] = {
            "tabs": [],
            "cards": [],
            "links": [],
            "buttons": [],
            "forms": [],
            "tables": [],
            "lists": [],
            "images": [],
            "pagination": [],
        }
        result["dynamic_features"] = {
            "has_infinite_scroll": False,
            "has_lazy_loading": False,
            "has_modals": False,
            "has_tabs": False,
            "is_spa": False,
        }
        result["recommended_selectors"] = []

        try:
            with sync_playwright() as p:
                browser = p.chromium.launch(headless=headless)
                context = browser.new_context(
                    user_agent=user_agent,
                    viewport={"width": 1920, "height": 1080},
                )
                page = context.new_page()

                # í˜ì´ì§€ ë¡œë“œ
                print("[1/6] í˜ì´ì§€ ë¡œë”© ì¤‘...")
                page.goto(url, wait_until="domcontentloaded", timeout=timeout)
                page.wait_for_timeout(3000)

                # í˜ì´ì§€ ê¸°ë³¸ ì •ë³´
                print("[2/6] í˜ì´ì§€ ì •ë³´ ìˆ˜ì§‘ ì¤‘...")
                result["page_info"] = {
                    "title": page.title(),
                    "url": page.url,
                    "has_changed_url": page.url != url,
                }

                # íƒ­/ë„¤ë¹„ê²Œì´ì…˜ íƒìƒ‰
                print("[3/6] íƒ­/ë„¤ë¹„ê²Œì´ì…˜ íƒìƒ‰ ì¤‘...")
                for sel in TAB_SELECTORS:
                    try:
                        tabs = page.query_selector_all(sel)
                        if tabs and len(tabs) > 0:
                            result["selectors"]["tabs"].append(
                                {
                                    "selector": sel,
                                    "count": len(tabs),
                                    "texts": [t.inner_text().strip()[:50] for t in tabs[:5]],
                                }
                            )
                            result["dynamic_features"]["has_tabs"] = True
                    except Exception:
                        pass

                # ì¹´ë“œ/ì•„ì´í…œ íƒìƒ‰
                print("[4/6] ì¹´ë“œ/ë¦¬ìŠ¤íŠ¸ ì•„ì´í…œ íƒìƒ‰ ì¤‘...")
                for sel in CARD_SELECTORS:
                    try:
                        cards = page.query_selector_all(sel)
                        if cards and len(cards) >= 2:
                            first_card = cards[0]
                            card_info = {
                                "selector": sel,
                                "count": len(cards),
                                "sample_text": first_card.inner_text().strip()[:100],
                                "has_link": first_card.query_selector("a") is not None,
                                "has_image": first_card.query_selector("img") is not None,
                                "data_attributes": [],
                            }
                            for attr in ["data-id", "data-category", "data-type", "data-level"]:
                                val = first_card.get_attribute(attr)
                                if val:
                                    card_info["data_attributes"].append({attr: val})
                            result["selectors"]["cards"].append(card_info)
                    except Exception:
                        pass

                # ë§í¬ íƒìƒ‰
                print("[5/6] ì£¼ìš” ë§í¬ íƒìƒ‰ ì¤‘...")
                try:
                    links = page.query_selector_all("a[href]")
                    unique_patterns = set()
                    for link in links[:50]:
                        href = link.get_attribute("href") or ""
                        text = link.inner_text().strip()[:30]
                        if href and not href.startswith("#") and not href.startswith("javascript"):
                            pattern = re.sub(r"/[a-zA-Z0-9_-]{10,}", "/*", href)
                            pattern = re.sub(r"/\\d+", "/*", pattern)
                            if pattern not in unique_patterns and len(unique_patterns) < 10:
                                unique_patterns.add(pattern)
                                result["selectors"]["links"].append({"pattern": pattern, "sample_href": href[:100], "sample_text": text})
                except Exception:
                    pass

                # ë²„íŠ¼ íƒìƒ‰
                try:
                    buttons = page.query_selector_all("button, [role='button'], .btn, [class*='button']")
                    for btn in buttons[:10]:
                        text = btn.inner_text().strip()[:30]
                        if text:
                            result["selectors"]["buttons"].append({"text": text, "tag": btn.evaluate("el => el.tagName"), "classes": btn.get_attribute("class") or ""})
                except Exception:
                    pass

                # í˜ì´ì§€ë„¤ì´ì…˜ íƒìƒ‰
                try:
                    for sel in PAGINATION_SELECTORS:
                        pag = page.query_selector(sel)
                        if pag:
                            result["selectors"]["pagination"].append({"selector": sel, "exists": True})
                except Exception:
                    pass

                # ë™ì  ê¸°ëŠ¥ ê°ì§€
                print("[6/6] ë™ì  ê¸°ëŠ¥ ê°ì§€ ì¤‘...")
                try:
                    spa_indicators = page.evaluate(
                        """() => {
                            return {
                                hasReact: !!document.querySelector('[data-reactroot], [data-reactid], #__next'),
                                hasVue: !!document.querySelector('[data-v-], #app[data-v-app]'),
                                hasAngular: !!document.querySelector('[ng-app], [ng-controller], app-root')
                            }
                        }"""
                    )
                    if any(spa_indicators.values()):
                        result["dynamic_features"]["is_spa"] = True
                        result["dynamic_features"]["spa_framework"] = spa_indicators

                    has_scroll = page.evaluate("""() => document.body.scrollHeight > window.innerHeight * 2""")
                    result["dynamic_features"]["has_infinite_scroll"] = has_scroll

                    for sel in MODAL_SELECTORS:
                        if page.query_selector(sel):
                            result["dynamic_features"]["has_modals"] = True
                            break
                except Exception:
                    pass

                screenshot_path = output_path.replace(".json", "_screenshot.png")
                page.screenshot(path=screenshot_path, full_page=False)
                result["screenshot"] = screenshot_path

                if result["selectors"]["cards"]:
                    best_card = max(result["selectors"]["cards"], key=lambda x: x["count"])
                    result["recommended_selectors"].append({"type": "card", "selector": best_card["selector"], "reason": f"{best_card['count']}ê°œ ë°œê²¬, ë°˜ë³µ íŒ¨í„´"})

                if result["selectors"]["tabs"]:
                    best_tab = result["selectors"]["tabs"][0]
                    result["recommended_selectors"].append({"type": "tab", "selector": best_tab["selector"], "reason": f"{best_tab['count']}ê°œ íƒ­ ë°œê²¬"})

                browser.close()
                result["status"] = "success"

        except Exception as e:
            result["status"] = "error"
            result["errors"].append(str(e))
            print(f"[ERROR] {e}")

        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(result, f, ensure_ascii=False, indent=2)

        print(f"\n{'='*60}")
        print("íƒìƒ‰ ì™„ë£Œ!")
        print(f"{'='*60}")
        print("\n[ê²°ê³¼ ìš”ì•½]")
        print(f"  - ìƒíƒœ: {result['status']}")
        print(f"  - ë¶„ì„ ë°©ë²•: {result.get('analysis_method')}")
        print(f"  - íƒ­: {len(result['selectors']['tabs'])}ê°œ íŒ¨í„´")
        print(f"  - ì¹´ë“œ: {len(result['selectors']['cards'])}ê°œ íŒ¨í„´")
        print(f"  - ë§í¬ íŒ¨í„´: {len(result['selectors']['links'])}ê°œ")
        print(f"  - SPA ì—¬ë¶€: {result['dynamic_features'].get('is_spa')}")
        print("\n[ì¶œë ¥ íŒŒì¼]")
        print(f"  - JSON: {output_path}")
        print(f"  - ìŠ¤í¬ë¦°ìƒ·: {result.get('screenshot', 'N/A')}")

        if result["recommended_selectors"]:
            print("\n[ì¶”ì²œ ì…€ë ‰í„°]")
            for rec in result["recommended_selectors"]:
                print(f"  - {rec['type']}: {rec['selector']} ({rec['reason']})")

        return result

    raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” mode: {mode}")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="ì •ì  HTML/RSS/Playwright ê¸°ë°˜ìœ¼ë¡œ í˜ì´ì§€ êµ¬ì¡°ë¥¼ íƒìƒ‰í•˜ê³  êµ¬ì¡° JSONì„ ìƒì„±í•©ë‹ˆë‹¤.",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog=r"""
ì˜ˆì‹œ:
  python explore_template.py https://example.com
  python explore_template.py https://example.com .\temp\91_temp_create_files\example_structure.json
  python explore_template.py https://example.com --mode static
  python explore_template.py https://example.com --mode playwright --headless false
        """,
    )

    parser.add_argument("url", help="íƒìƒ‰í•  URL")
    parser.add_argument("output_path", nargs="?", default=None, help="ê²°ê³¼ JSON ì €ì¥ ê²½ë¡œ(ì„ íƒ)")
    parser.add_argument("--mode", choices=["auto", "static", "rss", "playwright"], default="auto", help="íƒìƒ‰ ëª¨ë“œ (ê¸°ë³¸: auto)")
    parser.add_argument("--timeout", type=int, default=30000, help="íƒ€ì„ì•„ì›ƒ(ms) (ê¸°ë³¸: 30000)")
    parser.add_argument("--headless", action=argparse.BooleanOptionalAction, default=True, help="Playwright headless (ê¸°ë³¸: true)")
    parser.add_argument("--user-agent", default=default_user_agent(), help="HTTP User-Agent (ê¸°ë³¸: Windows Chrome UA)")
    parser.add_argument("--skip-robots-prompt", action="store_true", help="robots.txt ì°¨ë‹¨ ì‹œ ì‚¬ìš©ì 2íšŒ í™•ì¸ì„ ìƒëµí•˜ê³  ê³„ì† ì§„í–‰")

    args = parser.parse_args()

    explore_page_structure(
        args.url,
        output_path=args.output_path,
        headless=args.headless,
        timeout=args.timeout,
        mode=args.mode,
        user_agent=args.user_agent,
        skip_robots_prompt=args.skip_robots_prompt,
    )
